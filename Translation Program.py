import numpy as npimport typingfrom typing import Any, Tupleimport tensorflow as tfimport tensorflow_ranking as tfrimport tensorflow_text as tf_textimport matplotlib.pyplot as pltimport matplotlib.ticker as ticker#Loading Data# this function:     # adds a start and end token to each sentence,     # removes all the special characters    # Construct a word index and reverse word index (allows a word to be found by an id and an ide to be found from a word)    # Pad each sentence to a maximum def load_data(path):  # text = path.read_text(encoding='utf-8')  text = path.read_text(encoding = 'latin-1')  lines = text.splitlines()  # pairs = [line.split('\t') for line in lines]  pairs = [line.split('\n') for line in lines]  # inp = [inp for targ, inp in pairs]  # targ = [targ for targ, inp in pairs]  targ = []  inp = []  for i in pairs:      if (i != ['']):           findspeak = i[0].split(",")           if findspeak[0] == "INTERVIEWER":               inp.append(findspeak[1].replace('"', ""))           else:               targ.append(findspeak[1].replace('"', ""))  return targ, inp#Text Preprocessing#Standardization: standardizes input text# first by removing accented words and replacing them with its ascii equivalentdef tf_lower_and_split_punct(text):  # Split accecented characters.  text = tf_text.normalize_utf8(text, 'NFKD')  text = tf.strings.lower(text)  # Keep space, a to z, and select punctuation.  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')  # Add spaces around punctuation.  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \0 ')  # Strip whitespace.  text = tf.strings.strip(text)  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')  return text# The Encoder# the encoder takes the list of tokenIDs# then looks  up an embedding vector for each token# processes the embeddings into a new sequence# returning:     # a processed sequence (To be passed to the Attention head)    # Internal state (Used to initialise the decoder)class Encoder(tf.keras.layers.Layer):  def __init__(self, input_vocab_size, embedding_dim, enc_units):    super(Encoder, self).__init__()    self.enc_units = enc_units    self.input_vocab_size = input_vocab_size    # The embedding layer converts tokens to vectors    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,                                               embedding_dim)    # The GRU RNN layer processes those vectors sequentially.    self.gru = tf.keras.layers.GRU(self.enc_units,                                   # Return the sequence and state                                   return_sequences=True,                                   return_state=True,                                   recurrent_initializer='glorot_uniform')  def call(self, tokens, state=None):    # 2. The embedding layer looks up the embedding for each token.    vectors = self.embedding(tokens)    # 3. The GRU processes the embedding sequence.    #    output shape: (batch, s, enc_units)    #    state shape: (batch, enc_units)    output, state = self.gru(vectors, initial_state=state)    # 4. Returns the new sequence and its state.    return output, state# The attention head# The decoder uses attention to selectively focus on parts of the input sequence# The attention takes a series of vectors as input for each example and returns an attention vector for each exampleclass BahdanauAttention(tf.keras.layers.Layer):  def __init__(self, units):    super().__init__()    # For Eqn. (4), the  Bahdanau attention    self.W1 = tf.keras.layers.Dense(units, use_bias=False)    self.W2 = tf.keras.layers.Dense(units, use_bias=False)    self.attention = tf.keras.layers.AdditiveAttention()  def call(self, query, value, mask):     # From Eqn. (4), `W1@ht`.     w1_query = self.W1(query)     # From Eqn. (4), `W2@hs`.     w2_key = self.W2(value)     query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)     value_mask = mask     context_vector, attention_weights = self.attention(         inputs = [w1_query, value, w2_key],         mask=[query_mask, value_mask],         return_attention_scores = True,     )     return context_vector, attention_weights# The Decoder# The decoder will generate predictions for the next output token# it is given the complete output of the encoder# using an RNN it keeps track of what it has generated so far# the RNN output is used as a query to the attention over the encoders output, producing thecontext vector# A combination of the context vector and the RNN output is used to generate the attention vector# it generates logit predictions for the next based on the attention vectorclass Decoder(tf.keras.layers.Layer):  def __init__(self, output_vocab_size, embedding_dim, dec_units):    super(Decoder, self).__init__()    self.dec_units = dec_units    self.output_vocab_size = output_vocab_size    self.embedding_dim = embedding_dim    # For Step 1. The embedding layer convets token IDs to vectors    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,                                               embedding_dim)    # For Step 2. The RNN keeps track of what's been generated so far.    self.gru = tf.keras.layers.GRU(self.dec_units,                                   return_sequences=True,                                   return_state=True,                                   recurrent_initializer='glorot_uniform')    # For step 3. The RNN output will be the query for the attention layer.    self.attention = BahdanauAttention(self.dec_units)    # For step 4. Eqn. (3): converting `ct` to `at`    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,                                    use_bias=False)    # For step 5. This fully connected layer produces the logits for each    # output token.    self.fc = tf.keras.layers.Dense(self.output_vocab_size)    class DecoderInput(typing.NamedTuple):  new_tokens: Any  enc_output: Any  mask: Anyclass DecoderOutput(typing.NamedTuple):  logits: Any  attention_weights: Anydef call(self,         inputs: DecoderInput,         state=None) -> Tuple[DecoderOutput, tf.Tensor]:              # Step 1. Lookup the embeddings          vectors = self.embedding(inputs.new_tokens)                  # Step 2. Process one step with the RNN          rnn_output, state = self.gru(vectors, initial_state=state)                      # Step 3. Use the RNN output as the query for the attention over the          # encoder output.          context_vector, attention_weights = self.attention(              query=rnn_output, value=inputs.enc_output, mask=inputs.mask)                  # Step 4. Eqn. (3): Join the context_vector and rnn_output          #     [ct; ht] shape: (batch t, value_units + query_units)          context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)                  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`          attention_vector = self.Wc(context_and_rnn_output)                  # Step 5. Generate logit predictions:          logits = self.fc(attention_vector)          return DecoderOutput(logits, attention_weights), stateclass MaskedLoss(tf.keras.losses.Loss):  def __init__(self):    self.name = 'masked_loss'    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(        from_logits=True, reduction='none')  def __call__(self, y_true, y_pred):    # Calculate the loss for each item in the batch.    loss = self.loss(y_true, y_pred)    # Mask off the losses on padding.    mask = tf.cast(y_true != 0, tf.float32)    loss *= mask    # Return the total.    return tf.reduce_sum(loss)# Implementing the Training Stepclass TrainTranslator(tf.keras.Model):  def __init__(self, embedding_dim, units,               input_text_processor,               output_text_processor,                use_tf_function=True):    super().__init__()    # Build the encoder and decoder    encoder = Encoder(input_text_processor.vocabulary_size(),                      embedding_dim, units)    decoder = Decoder(output_text_processor.vocabulary_size(),                      embedding_dim, units)    self.encoder = encoder    self.decoder = decoder    self.input_text_processor = input_text_processor    self.output_text_processor = output_text_processor    self.use_tf_function = use_tf_function  def train_step(self, inputs):    if self.use_tf_function:      return self._tf_train_step(inputs)    else:      return self._train_step(inputs)def _preprocess(self, input_text, target_text):        # Convert the text to token IDs    input_tokens = self.input_text_processor(input_text)    target_tokens = self.output_text_processor(target_text)        # Convert IDs to masks.    input_mask = input_tokens != 0        target_mask = target_tokens != 0        return input_tokens, input_mask, target_tokens, target_maskTrainTranslator._preprocess = _preprocessdef _train_step(self, inputs):  input_text, target_text = inputs    (input_tokens, input_mask,   target_tokens, target_mask) = self._preprocess(input_text, target_text)  max_target_length = tf.shape(target_tokens)[1]  with tf.GradientTape() as tape:    # Encode the input    enc_output, enc_state = self.encoder(input_tokens)    # Initialize the decoder's state to the encoder's final state.    # This only works if the encoder and decoder have the same number of    # units.    dec_state = enc_state    loss = tf.constant(0.0)    for t in tf.range(max_target_length-1):      # Pass in two tokens from the target sequence:      # 1. The current input to the decoder.      # 2. The target for the decoder's next prediction.      new_tokens = target_tokens[:, t:t+2]      step_loss, dec_state = self._loop_step(new_tokens, input_mask,                                             enc_output, dec_state)      loss = loss + step_loss    # Average the loss over all non padding tokens.    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))  # Apply an optimization step  variables = self.trainable_variables   gradients = tape.gradient(average_loss, variables)  self.optimizer.apply_gradients(zip(gradients, variables))  # Return a dict mapping metric names to current value  return {'batch_loss': average_loss}def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]  # Run the decoder one step.  decoder_input = DecoderInput(new_tokens=input_token,                               enc_output=enc_output,                               mask=input_mask)  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)  # `self.loss` returns the total for non-padded tokens  y = target_token  y_pred = dec_result.logits  step_loss = self.loss(y, y_pred)  return step_loss, dec_state@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),                               tf.TensorSpec(dtype=tf.string, shape=[None])]])def _tf_train_step(self, inputs):  return self._train_step(inputs)# Train Modelclass BatchLogs(tf.keras.callbacks.Callback):  def __init__(self, key):    self.key = key    self.logs = []  def on_train_batch_end(self, n, logs):    self.logs.append(logs[self.key])# Translate# similar to  the training Loop except the input to decoder at each time step is a sample from the decoders last predictionclass Translator(tf.Module):  def __init__(self, encoder, decoder, input_text_processor,               output_text_processor):    self.encoder = encoder    self.decoder = decoder    self.input_text_processor = input_text_processor    self.output_text_processor = output_text_processor    self.output_token_string_from_index = (        tf.keras.layers.StringLookup(            vocabulary=output_text_processor.get_vocabulary(),            mask_token='',            invert=True))    # The output should never generate padding, unknown, or start.    index_from_string = tf.keras.layers.StringLookup(        vocabulary=output_text_processor.get_vocabulary(), mask_token='')    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)    token_mask[np.array(token_mask_ids)] = True    self.token_mask = token_mask    self.start_token = index_from_string(tf.constant('[START]'))    self.end_token = index_from_string(tf.constant('[END]'))# Convert tokens IDs to textdef tokens_to_text(self, result_tokens):  result_text_tokens = self.output_token_string_from_index(result_tokens)  result_text = tf.strings.reduce_join(result_text_tokens,                                       axis=1, separator=' ')  result_text = tf.strings.strip(result_text)  return result_text# Sample from the decoders predictionsdef sample(self, logits, temperature):  # Set the logits for all masked tokens to -inf, so they are never chosen.  logits = tf.where(self.token_mask, -np.inf, logits)  if temperature == 0.0:    new_tokens = tf.argmax(logits, axis=-1)  else:     logits = tf.squeeze(logits, axis=1)    new_tokens = tf.random.categorical(logits/temperature,                                        num_samples=1)  return new_tokens# Implement the translation loopdef translate_unrolled(self,                       input_text, *,                       max_length=50,                       return_attention=True,                       temperature=1.0):  batch_size = tf.shape(input_text)[0]  input_tokens = self.input_text_processor(input_text)  enc_output, enc_state = self.encoder(input_tokens)  dec_state = enc_state  new_tokens = tf.fill([batch_size, 1], self.start_token)  result_tokens = []  attention = []  done = tf.zeros([batch_size, 1], dtype=tf.bool)  for _ in range(max_length):    dec_input = DecoderInput(new_tokens=new_tokens,                             enc_output=enc_output,                             mask=(input_tokens!=0))        dec_result, dec_state = self.decoder(dec_input, state=dec_state)    attention.append(dec_result.attention_weights)    new_tokens = self.sample(dec_result.logits, temperature)    # If a sequence produces an `end_token`, set it `done`    done = done | (new_tokens == self.end_token)    # Once a sequence is done it only produces 0-padding.    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)    # Collect the generated tokens    result_tokens.append(new_tokens)        if tf.executing_eagerly() and tf.reduce_all(done):      break  # Convert the list of generates token ids to a list of strings.  result_tokens = tf.concat(result_tokens, axis=-1)  result_text = self.tokens_to_text(result_tokens)  print(result_tokens)  print(result_text)  if return_attention:    attention_stack = tf.concat(attention, axis=1)    return {'text': result_text, 'attention': attention_stack}  else:    return {'text': result_text}def translate_symbolic(self,                       input_text,                       *,                       max_length=50,                       return_attention=True,                       temperature=1.0):  batch_size = tf.shape(input_text)[0]  # Encode the input  input_tokens = self.input_text_processor(input_text)  enc_output, enc_state = self.encoder(input_tokens)  # Initialize the decoder  dec_state = enc_state  new_tokens = tf.fill([batch_size, 1], self.start_token)  # Initialize the accumulators  result_tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)  attention = tf.TensorArray(tf.float32, size=1, dynamic_size=True)  done = tf.zeros([batch_size, 1], dtype=tf.bool)  for t in tf.range(max_length):    dec_input = DecoderInput(        new_tokens=new_tokens, enc_output=enc_output, mask=(input_tokens != 0))    dec_result, dec_state = self.decoder(dec_input, state=dec_state)    attention = attention.write(t, dec_result.attention_weights)    new_tokens = self.sample(dec_result.logits, temperature)    # If a sequence produces an `end_token`, set it `done`    done = done | (new_tokens == self.end_token)    # Once a sequence is done it only produces 0-padding.    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)    # Collect the generated tokens    result_tokens = result_tokens.write(t, new_tokens)    if tf.reduce_all(done):      break  # Convert the list of generated token ids to a list of strings.  result_tokens = result_tokens.stack()  result_tokens = tf.squeeze(result_tokens, -1)  result_tokens = tf.transpose(result_tokens, [1, 0])  result_text = self.tokens_to_text(result_tokens)  if return_attention:    attention_stack = attention.stack()    attention_stack = tf.squeeze(attention_stack, 2)    attention_stack = tf.transpose(attention_stack, [1, 0, 2])    return {'text': result_text, 'attention': attention_stack}  else:    return {'text': result_text}def plot_attention(attention, sentence, predicted_sentence):  sentence = tf_lower_and_split_punct(sentence).numpy().decode().split()  predicted_sentence = predicted_sentence.numpy().decode().split() + ['[END]']  fig = plt.figure(figsize=(10, 10))  ax = fig.add_subplot(1, 1, 1)  attention = attention[:len(predicted_sentence), :len(sentence)]  ax.matshow(attention, cmap='viridis', vmin=0.0)  fontdict = {'fontsize': 14}  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))  ax.set_xlabel('Input text')  ax.set_ylabel('Output text')  plt.suptitle('Attention weights')  def calcMRR(model,question,responses,ansindex):    mrr = tfr.keras.metrics.MRRMetric()    # %% Data importing and Vectorisation use_builtins = True#Download the datasetimport pathlibpath_to_file = pathlib.Path()/"Johnny Depp.txt"targ, inp = load_data(path_to_file)#Create TF Dataset# shuffles the data and batches them efficientlyBUFFER_SIZE = len(inp)BATCH_SIZE = 64dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)dataset = dataset.batch(BATCH_SIZE)# Text Vectorisation# Standardisation is wrapped in a text vectorization layer which handles vocab extraction and conversion of inputs to sequence tokensmax_vocab_size = 5000input_text_processor = tf.keras.layers.TextVectorization(    standardize=tf_lower_and_split_punct,    max_tokens=max_vocab_size)input_text_processor.adapt(inp)output_text_processor = tf.keras.layers.TextVectorization(    standardize=tf_lower_and_split_punct,    max_tokens=max_vocab_size)output_text_processor.adapt(targ)# %% setting up Encoder and Decoder# Encoder and Decoder Modelembedding_dim = 256units = 1024Decoder.call = call# %% TrainingTrainTranslator._train_step = _train_stepTrainTranslator._loop_step = _loop_step#  Test the Training Steptranslator = TrainTranslator(    embedding_dim, units,    input_text_processor=input_text_processor,    output_text_processor=output_text_processor,    use_tf_function=False)# Configure the loss and optimizertranslator.compile(    optimizer=tf.optimizers.Adam(),    loss=MaskedLoss(),    metrics=[tfr.keras.metrics.MRRMetric()])np.log(output_text_processor.vocabulary_size())TrainTranslator._tf_train_step = _tf_train_steptranslator.use_tf_function = Truetrain_translator = TrainTranslator(    embedding_dim, units,    input_text_processor=input_text_processor,    output_text_processor=output_text_processor)# Configure the loss and optimizertrain_translator.compile(    optimizer=tf.optimizers.Adam(),    loss=MaskedLoss(),)batch_loss = BatchLogs('batch_loss')# train_translator.fit(dataset, epochs=5, workers = 5, callbacks=[batch_loss])train_translator.fit(dataset, epochs=1, workers = 5, callbacks=[batch_loss])plt.plot(batch_loss.logs)plt.ylim([0, 3])plt.xlabel('Batch #')plt.ylabel('CE/token')# %% Input parsingtranslator = Translator(    encoder=train_translator.encoder,    decoder=train_translator.decoder,    input_text_processor=input_text_processor,    output_text_processor=output_text_processor,)Translator.tokens_to_text = tokens_to_textTranslator.sample = sampleTranslator.translate = translate_unrolled# Translator.translate = translate_symbolicinput_text = tf.constant([    # It is yes,    'Its a very unusual film youll agree with that',])result = translator.translate(input_text)print("Rawr Output: ")print(result)print("Decoded output: ")print(result["text"].numpy()[0].decode())print()# Visualise the Processa = result['attention'][0]# print(np.sum(a, axis=-1))_ = plt.bar(range(len(a[0, :])), a[0, :])plt.imshow(np.array(a), vmin=0.0)plot_attention(result['attention'][0], input_text[0], result['text'][0])# %% MRRquestion = tf.constant([    "I mean you know  we’re being silly about this but you really could have been seriously seriously injured if not killed,",])responses = tf.constant([    "Horrifically mangled at the very least",    "Quite Badly",    "Horse jack Man",    "Man Cat Dog"     ])calcMRR(translator,question,responses,1)